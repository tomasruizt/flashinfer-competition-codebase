{
  "name": "fla-recurrent",
  "definition": "gdn_decode_qk4_v8_d128_k_last",
  "author": "lmu-css",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "cuda"
    ],
    "entry_point": "kernel.py::kernel_fla_recurrent",
    "dependencies": [],
    "destination_passing_style": true,
    "binding": null
  },
  "sources": [
    {
      "path": "kernel.py",
      "content": "import math\nimport torch\nimport torch.nn.functional as F\nimport triton\nfrom fla.ops.gated_delta_rule.fused_recurrent import (\n    fused_recurrent_gated_delta_rule_fwd_kernel,\n)\n\n\n# ---------------------------------------------------------------------------\n# PyTorch reference implementation (for correctness checking / understanding)\n# ---------------------------------------------------------------------------\n\n\ndef _reference_decode(q, k, v, state, A_log, a, dt_bias, b, scale):\n    \"\"\"\n    Gated Delta Net decode reference implementation (k-last layout).\n\n    Single-token recurrent step: reads from a fixed-size state matrix using q,\n    then updates the state by erasing old information at key k and writing new\n    value v. GVA config: q/k heads are repeat-interleaved to match v heads.\n\n    Args:\n        q:       [B, 1, num_q_heads=4, K=128]  bf16 — query, used to read from state.\n        k:       [B, 1, num_k_heads=4, K=128]  bf16 — key, the \"address\" to erase/write in state.\n        v:       [B, 1, num_v_heads=8, V=128]  bf16 — value, new information to write into state.\n        state:   [B, num_v_heads=8, V=128, K=128] f32 — recurrent state (k-last layout [H,V,K]).\n                 Optional; zeros if None.\n        A_log:   [num_v_heads=8]                f32 — learnable log-space base decay rate per head.\n                 Controls how fast each head forgets: g = exp(-exp(A_log) * softplus(a + dt_bias)).\n        a:       [B, 1, num_v_heads=8]          bf16 — input-dependent decay (per token, per head).\n        dt_bias: [num_v_heads=8]                f32 — learnable decay bias, added to a before softplus.\n        b:       [B, 1, num_v_heads=8]          bf16 — update gate input. beta = sigmoid(b) controls\n                 how much of the new value to write vs retaining old value at key k.\n        scale:   scalar f32 — output scale factor, default 1/sqrt(K) = 1/sqrt(128).\n\n    Returns:\n        output:    [B, 1, num_v_heads=8, V=128] bf16 — attention output (scale * q @ state_new).\n        new_state: [B, num_v_heads=8, V=128, K=128] f32 — updated recurrent state (k-last layout).\n\n    Recurrence (per head, working in [K,V] layout):\n        g     = exp(-exp(A_log) * softplus(a + dt_bias))   # global decay ∈ (0,1)\n        beta  = sigmoid(b)                                  # update gate ∈ (0,1)\n        S_old = g * S                                       # decay entire state\n        S_new = S_old - k^T @ (k @ S_old) + k^T @ (beta * v + (1-beta) * (k @ S_old))\n        out   = scale * q @ S_new\n    \"\"\"\n    B, T, num_q_heads, K = q.shape\n    _, _, num_k_heads, _ = k.shape\n    _, _, num_v_heads, V = v.shape\n    num_heads = num_v_heads\n    device = q.device\n\n    assert num_q_heads == 4\n    assert num_k_heads == 4\n    assert num_v_heads == 8\n    assert K == 128 and V == 128\n    assert T == 1\n\n    if scale is None or scale == 0.0:\n        scale = 1.0 / math.sqrt(K)\n\n    # Compute g and beta from raw parameters\n    x = a.float() + dt_bias.float()  # [B, 1, HV]\n    g = torch.exp(-torch.exp(A_log.float()) * F.softplus(x))  # [B, 1, HV]\n    beta = torch.sigmoid(b.float())  # [B, 1, HV]\n\n    q_f32 = q.squeeze(1).float()\n    k_f32 = k.squeeze(1).float()\n    v_f32 = v.squeeze(1).float()\n    g_f32 = g.squeeze(1).float()\n    beta_f32 = beta.squeeze(1).float()\n\n    if state is not None:\n        state_f32 = state.float()\n    else:\n        state_f32 = torch.zeros(B, num_heads, V, K, dtype=torch.float32, device=device)\n\n    q_exp = q_f32.repeat_interleave(num_v_heads // num_q_heads, dim=1)\n    k_exp = k_f32.repeat_interleave(num_v_heads // num_k_heads, dim=1)\n\n    new_state = torch.zeros_like(state_f32)\n    output = torch.zeros(B, num_heads, V, dtype=torch.float32, device=device)\n\n    # fmt: off\n    for b_idx in range(B):\n        for h_idx in range(num_heads):\n            q_h = q_exp[b_idx, h_idx]       # [K=128]        — query vector\n            k_h = k_exp[b_idx, h_idx]       # [K=128]        — key vector (L2-normalized)\n            v_h = v_f32[b_idx, h_idx]       # [V=128]        — value vector\n            h_state = (\n                state_f32[b_idx, h_idx].clone().transpose(-1, -2)\n            )                                # [K=128, V=128] — state transposed from [V,K] storage\n            g_val = g_f32[b_idx, h_idx]     # scalar          — global decay gate ∈ (0,1)\n            beta_val = beta_f32[b_idx, h_idx]  # scalar       — update gate ∈ (0,1)\n\n            old_state = g_val * h_state     # [K, V]          — decayed state\n            old_v = k_h @ old_state         # [K] @ [K, V] -> [V] — matvec: value currently stored at key k\n            new_v = beta_val * v_h + (1 - beta_val) * old_v  # [V] — blended new/old value\n            state_remove = k_h.unsqueeze(1) @ old_v.unsqueeze(0)  # [K,1] @ [1,V] -> [K, V] — outer product: erase old\n            state_update = k_h.unsqueeze(1) @ new_v.unsqueeze(0)  # [K,1] @ [1,V] -> [K, V] — outer product: write new\n            h_state = old_state - state_remove + state_update     # [K, V] — updated state\n\n            output[b_idx, h_idx] = scale * (q_h @ h_state)  # [K] @ [K, V] -> [V] — read from state\n            new_state[b_idx, h_idx] = h_state.transpose(-1, -2)  # [K,V] -> [V,K] back to storage layout\n    # fmt: on\n\n    output = output.unsqueeze(1).to(torch.bfloat16)\n    return output, new_state\n\n\n# ---------------------------------------------------------------------------\n# FLA fused recurrent wrapper (optimized Triton kernel)\n# ---------------------------------------------------------------------------\n\n\ndef _fla_decode(q, k, v, state, A_log, a, dt_bias, b, scale):\n    \"\"\"\n    Wrapper adapting our competition interface to FLA's fused_recurrent_gated_delta_rule_fwd_kernel.\n    Calls the Triton kernel directly instead of fused_recurrent_gated_delta_rule_fwd().\n\n    Interface differences:\n        Ours                              FLA kernel\n        ----                              ----------\n        A_log, a, dt_bias → g (decay)     g: [B, T, HV] log-space decay (pre-computed)\n        b → beta (update gate)            beta: [B, T, HV] already sigmoid'd\n        state: [B, H, V, K] (k-last)     h0: [B, H, K, V] (k-first)\n        q: [B, T, num_q_heads=4, K]      q: [B, T, H=4, K]  (same)\n        k: [B, T, num_k_heads=4, K]      k: [B, T, H=4, K]  (same)\n        v: [B, T, num_v_heads=8, V]      v: [B, T, HV=8, V] (same)\n\n    Returns:\n        output:    [B, 1, 8, 128] bf16\n        new_state: [B, 8, 128, 128] f32 (k-last layout)\n    \"\"\"\n    K = q.shape[-1]\n    B, T, H, _ = k.shape\n    V = v.shape[-1]\n    HV = v.shape[2]\n\n    if scale is None or scale == 0.0:\n        scale = 1.0 / math.sqrt(K)\n\n    # Compute log-space decay: g = log(decay) = -exp(A_log) * softplus(a + dt_bias)\n    # FLA kernel does: h *= exp(g), so g must be log of the decay factor\n    g = -torch.exp(A_log.float()) * F.softplus(a.float() + dt_bias.float())  # [B, 1, 8]\n\n    # Compute beta = sigmoid(b)\n    beta = torch.sigmoid(b.float())  # [B, 1, 8]\n\n    # Transpose state from k-last [B, H, V, K] to k-first [B, H, K, V] for FLA\n    if state is not None:\n        initial_state = state.float().transpose(-1, -2).contiguous()  # [B, 8, K, V]\n    else:\n        initial_state = None\n\n    # Same allocation and grid logic as fused_recurrent_gated_delta_rule_fwd()\n    N = B\n    BK = triton.next_power_of_2(K)\n    BV = min(8, triton.next_power_of_2(V))  # gv is None\n    NV = triton.cdiv(V, BV)\n\n    o = torch.empty_like(v)\n    final_state = q.new_empty(N, HV, K, V, dtype=torch.float32)\n\n    grid = (NV, N * HV)\n    fused_recurrent_gated_delta_rule_fwd_kernel[grid](\n        q=q,\n        k=k,\n        v=v,\n        g=g,\n        gk=None,\n        gv=None,\n        beta=beta,\n        o=o,\n        h0=initial_state,\n        ht=final_state,\n        cu_seqlens=None,\n        scale=scale,\n        T=T,\n        B=B,\n        H=H,\n        HV=HV,\n        K=K,\n        V=V,\n        BK=BK,\n        BV=BV,\n        IS_BETA_HEADWISE=True,\n        USE_QK_L2NORM_IN_KERNEL=False,\n        num_warps=1,\n        num_stages=3,\n    )\n\n    # o: [B, 1, 8, 128] — already correct shape and dtype\n    # final_state: [B, 8, K, V] → transpose back to k-last [B, 8, V, K]\n    new_state = final_state.transpose(-1, -2).contiguous()\n\n    return o.to(torch.bfloat16), new_state\n\n\n@torch.no_grad()\n@torch.compile(fullgraph=True)\ndef kernel_fla_recurrent(\n    q, k, v, state, A_log, a, dt_bias, b, scale, output, new_state\n):\n    \"\"\"DPS entry point: FLA fused recurrent Triton kernel.\"\"\"\n    out, ns = _fla_decode(q, k, v, state, A_log, a, dt_bias, b, scale)\n    output.copy_(out)\n    new_state.copy_(ns)\n\n\n@torch.no_grad()\n@torch.compile(fullgraph=True)\ndef kernel_pt_reference(q, k, v, state, A_log, a, dt_bias, b, scale, output, new_state):\n    \"\"\"DPS entry point: torch.compile'd PyTorch reference.\"\"\"\n    out, ns = _reference_decode(q, k, v, state, A_log, a, dt_bias, b, scale)\n    output.copy_(out)\n    new_state.copy_(ns)\n"
    }
  ],
  "description": ""
}